"""
ä¼æ¥­è²è­½æ™ºæ…§ç›£æ¸¬ç³»çµ± (FameKeeper)
================================
æ•´åˆå¤šå€‹è³‡æ–™æºï¼ˆRedditã€YouTubeã€Google Newsã€DuckDuckGoï¼‰
æ”¯æ´å¤šæ¨¡å‹åˆ‡æ›ï¼ˆGemini AI / OpenAIï¼‰èˆ‡åœ°å€ç¯©é¸

ä½œè€…ï¼šjkloip
ç‰ˆæœ¬ï¼š1.1.0 

"""

import streamlit as st
import pandas as pd
import sqlite3
from datetime import datetime
import time
import random
import importlib.util
import json
from urllib.parse import quote  # ç§»è‡³é ‚å±¤å¼•ç”¨

# === æ ¸å¿ƒå¥—ä»¶ï¼ˆå¿…è£ï¼‰ ===
try:
    from ddgs import DDGS
except ImportError:
    from duckduckgo_search import DDGS  # type: ignore
from google import genai
import feedparser

# OpenAI æ”¯æ´
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None  # type: ignore


# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šç³»çµ±é…ç½®èˆ‡åˆå§‹åŒ–
# ============================================================================

class PackageChecker:
    """æª¢æŸ¥å¯é¸å¥—ä»¶æ˜¯å¦å·²å®‰è£"""
    
    @staticmethod
    def is_installed(package_name: str) -> bool:
        return importlib.util.find_spec(package_name) is not None


class Config:
    """æ‡‰ç”¨ç¨‹å¼é…ç½®ç®¡ç†"""
    
    # æª¢æŸ¥å¯é¸å¥—ä»¶
    REDDIT_AVAILABLE = PackageChecker.is_installed('praw')
    YOUTUBE_AVAILABLE = PackageChecker.is_installed('googleapiclient')
    OPENAI_AVAILABLE = PackageChecker.is_installed('openai')
    
    # API Keysï¼ˆå°‡ç”± UI è¼¸å…¥ï¼‰
    GEMINI_API_KEY = ""
    OPENAI_API_KEY = ""
    
    REDDIT_CLIENT_ID = ""
    REDDIT_CLIENT_SECRET = ""
    REDDIT_USER_AGENT = "reputation_monitor/2.1"
    YOUTUBE_API_KEY = ""
    
    # è³‡æ–™åº«è¨­å®š
    DB_NAME = "reputation.db"
    
    # æ¨¡å‹è¨­å®š
    GEMINI_MODEL = "gemini-2.5-flash"
    OPENAI_MODEL = "gpt-4.1-mini" # ä½¿ç”¨è¼ƒç¶“æ¿Ÿå¯¦æƒ çš„æ¨¡å‹


# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šè³‡æ–™åº«ç®¡ç†
# ============================================================================

class DatabaseManager:
    """è³‡æ–™åº«æ“ä½œç®¡ç†é¡åˆ¥"""
    
    def __init__(self, db_name: str = Config.DB_NAME):
        self.db_name = db_name
        self.init_database()
    
    def init_database(self):
        with sqlite3.connect(self.db_name) as conn:
            cursor = conn.cursor()
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS mentions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    content TEXT NOT NULL,
                    platform TEXT NOT NULL,
                    sentiment_score REAL,
                    sentiment_label TEXT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    url TEXT,
                    author TEXT,
                    engagement INTEGER DEFAULT 0
                )
            ''')
            conn.commit()
    
    def save_mention(self, content: str, platform: str, score: float, 
                    label: str, url: str, author: str = "Unknown", 
                    engagement: int = 0):
        with sqlite3.connect(self.db_name) as conn:
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO mentions 
                (content, platform, sentiment_score, sentiment_label, 
                 timestamp, url, author, engagement) 
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (content, platform, score, label, datetime.now(), 
                  url, author, engagement))
            conn.commit()
    
    def get_recent_mentions(self, limit: int = 100) -> pd.DataFrame:
        with sqlite3.connect(self.db_name) as conn:
            query = "SELECT * FROM mentions ORDER BY timestamp DESC LIMIT ?"
            return pd.read_sql_query(query, conn, params=(limit,))
    
    def clear_all_data(self):
        with sqlite3.connect(self.db_name) as conn:
            conn.execute("DELETE FROM mentions")
            conn.commit()


# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šAI åˆ†ææœå‹™ (æ•´åˆ Gemini èˆ‡ OpenAI)
# ============================================================================

class UnifiedAIAnalyzer:
    """æ•´åˆ Gemini èˆ‡ OpenAI çš„åˆ†ææœå‹™"""
    
    def __init__(self, provider: str, api_key: str):
        self.provider = provider
        self.api_key = api_key
        self.client = None
        
        if self.provider == 'gemini':
            self.client = genai.Client(api_key=api_key) if api_key else None
            self.model = Config.GEMINI_MODEL
        elif self.provider == 'openai':
            if Config.OPENAI_AVAILABLE and api_key:
                self.client = OpenAI(api_key=api_key)
                self.model = Config.OPENAI_MODEL
            else:
                self.client = None

    def analyze_sentiment(self, text: str) -> tuple:
        """åˆ†ææ–‡å­—æƒ…æ„Ÿ"""
        if not self.client:
            return 0.0, "ä¸­ç«‹", "ç„¡æ³•åˆ†æï¼ˆæœªè¨­å®š API Key æˆ–å¥—ä»¶ç¼ºå¤±ï¼‰"
        
        prompt = self._build_sentiment_prompt(text[:500])
        
        try:
            if self.provider == 'gemini':
                return self._analyze_with_gemini(prompt)
            elif self.provider == 'openai':
                return self._analyze_with_openai(prompt)
        except Exception as e:
            error_str = str(e)
            if "quota" in error_str.lower() or "429" in error_str:
                return 0.0, "ä¸­ç«‹", "â— API é…é¡å·²ç”¨ç›¡"
            return 0.0, "ä¸­ç«‹", f"åˆ†æå¤±æ•—: {error_str[:50]}"
        
        return 0.0, "ä¸­ç«‹", "æœªçŸ¥éŒ¯èª¤"

    def _analyze_with_gemini(self, prompt: str) -> tuple:
        response = self.client.models.generate_content(
            model=self.model,
            contents=prompt
        )
        result = self._parse_json_response(response.text)
        return self._extract_result_tuple(result)

    def _analyze_with_openai(self, prompt: str) -> tuple:
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant that outputs JSON."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"}
        )
        result_text = response.choices[0].message.content
        result = self._parse_json_response(result_text)
        return self._extract_result_tuple(result)

    def generate_summary_report(self, df: pd.DataFrame) -> str:
        """ç”Ÿæˆæ•´é«”å ±å‘Š"""
        if not self.client or df.empty:
            return "æš«ç„¡æ•¸æ“šå¯åˆ†æ"
        
        # æº–å‚™è³‡æ–™
        sample_data = df.head(10)[['platform', 'sentiment_label', 'content']].to_dict('records')
        stats = self._calculate_statistics(df)
        prompt = self._build_summary_prompt(sample_data, stats)
        
        try:
            if self.provider == 'gemini':
                response = self.client.models.generate_content(
                    model=self.model,
                    contents=prompt
                )
                return response.text
            elif self.provider == 'openai':
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}]
                )
                return response.choices[0].message.content
        except Exception as e:
            return f"å ±å‘Šç”Ÿæˆå¤±æ•—: {str(e)[:100]}...\nè«‹æª¢æŸ¥ API Key æˆ–é…é¡ã€‚"

    # --- Helper Methods ---
    
    @staticmethod
    def _extract_result_tuple(result: dict) -> tuple:
        return (
            result.get("score", 0.0),
            result.get("label", "ä¸­ç«‹"),
            result.get("key_topic", "ç„¡æ³•æ‘˜è¦")
        )

    @staticmethod
    def _build_sentiment_prompt(text: str) -> str:
        return f"""è«‹åˆ†æä»¥ä¸‹æ–‡å­—çš„æƒ…æ„Ÿèˆ‡é—œéµè­°é¡Œï¼š
æ–‡å­—å…§å®¹ï¼š"{text}"
è«‹å‹™å¿…å›å‚³æ¨™æº– JSON æ ¼å¼ï¼ˆä¸è¦æœ‰ markdown æ¨™è¨˜ï¼‰ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "score": -1.0 åˆ° 1.0 ä¹‹é–“çš„æµ®é»æ•¸,
  "label": "æ­£é¢" æˆ– "è² é¢" æˆ– "ä¸­ç«‹",
  "key_topic": "ä¸€å¥è©±æ‘˜è¦ä¸»è¦è¨è«–çš„è­°é¡Œ"
}}"""
    
    @staticmethod
    def _build_summary_prompt(sample_data: list, stats: dict) -> str:
        return f"""ä½ æ˜¯ä¼æ¥­ã€å“ç‰Œèˆ‡å€‹äººè²è­½åˆ†æå°ˆå®¶ï¼Œæ ¹æ“šä»¥ä¸‹ç¤¾ç¾¤åª’é«”ç›£æ¸¬è³‡æ–™ï¼Œç”Ÿæˆä¸€ä»½å°ˆæ¥­çš„è²è­½æ‘˜è¦å ±å‘Šï¼ˆç´„ 150 å­—ï¼‰ï¼š
è³‡æ–™æ¨£æœ¬ï¼š{sample_data}
æ•´é«”çµ±è¨ˆï¼šå¹³å‡æƒ…æ„Ÿåˆ†æ•¸ï¼š{stats['avg_score']:.2f}, æ­£é¢ï¼š{stats['positive_count']}, è² é¢ï¼š{stats['negative_count']}, ä¸­ç«‹ï¼š{stats['neutral_count']}
è«‹æä¾›ï¼š1. æ•´é«”è²è­½è¶¨å‹¢åˆ¤æ–· 2. ä¸»è¦è¨è«–è­°é¡Œ 3. æ½›åœ¨é¢¨éšªæˆ–æ©Ÿæœƒé»"""
    
    @staticmethod
    def _parse_json_response(text: str) -> dict:
        try:
            clean_text = text.strip().replace("```json", "").replace("```", "")
            return json.loads(clean_text)
        except json.JSONDecodeError:
            return {}
    
    @staticmethod
    def _calculate_statistics(df: pd.DataFrame) -> dict:
        return {
            'avg_score': df['sentiment_score'].mean(),
            'positive_count': len(df[df['sentiment_label'] == 'æ­£é¢']),
            'negative_count': len(df[df['sentiment_label'] == 'è² é¢']),
            'neutral_count': len(df[df['sentiment_label'] == 'ä¸­ç«‹'])
        }


# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šè³‡æ–™æŠ“å–æœå‹™ (æ”¯æ´åœ°å€ç¯©é¸ + ç¨‹å¼ç¢¼å„ªåŒ–)
# ============================================================================

class DataFetcher:
    """è³‡æ–™æŠ“å–åŸºç¤é¡åˆ¥"""
    
    def __init__(self, analyzer: UnifiedAIAnalyzer):
        self.analyzer = analyzer
    
    def fetch(self, query: str, limit: int, region: str = "TW") -> list:
        raise NotImplementedError("å­é¡åˆ¥å¿…é ˆå¯¦ä½œæ­¤æ–¹æ³•")
    
    @staticmethod
    def _is_relevant(content: str, query: str, min_score: float = 0.3) -> bool:
        content_lower = content.lower()
        query_lower = query.lower()
        if query_lower in content_lower: return True
        
        keywords = query.replace('è‚¡ä»½æœ‰é™å…¬å¸', '').replace('å…¬å¸', '').strip()
        words = [w for w in keywords.split() if len(w) >= 2]
        if not words: words = [keywords]
        
        matched = sum(1 for word in words if word.lower() in content_lower)
        score = matched / len(words) if words else 0
        return score >= min_score
    
    def _create_result_dict(self, content: str, platform: str, url: str, 
                           author: str = "Unknown", engagement: int = 0) -> dict:
        score, label, topic = self.analyzer.analyze_sentiment(content)
        return {
            'content': content, 'platform': platform, 'score': score,
            'label': label, 'url': url, 'author': author, 'engagement': engagement
        }
    
    @staticmethod
    def _add_delay(min_sec: float = 1.0, max_sec: float = 2.0):
        time.sleep(random.uniform(min_sec, max_sec))


class RedditFetcher(DataFetcher):
    """Reddit è³‡æ–™æŠ“å–å™¨"""
    
    def __init__(self, analyzer, client_id, client_secret, user_agent):
        super().__init__(analyzer)
        import praw  # type: ignore
        self.reddit = praw.Reddit(
            client_id=client_id, client_secret=client_secret, user_agent=user_agent
        )
    
    def fetch(self, query: str, limit: int = 10, region: str = "TW") -> list:
        # Reddit æœå°‹ä¸å¼·åˆ¶å€åˆ† region (API é™åˆ¶)ï¼Œç¶­æŒä¸€èˆ¬æœå°‹
        results = []
        try:
            for submission in self.reddit.subreddit("all").search(query, limit=limit, sort="new"):
                content = f"{submission.title}\n{submission.selftext[:200]}"
                result = self._create_result_dict(
                    content=content, platform='Reddit',
                    url=f"https://reddit.com{submission.permalink}",
                    author=str(submission.author),
                    engagement=submission.score + submission.num_comments
                )
                results.append(result)
                self._add_delay(1, 2)
        except Exception as e:
            st.error(f"Reddit æŠ“å–å¤±æ•—: {e}")
        return results


class YouTubeFetcher(DataFetcher):
    """YouTube è³‡æ–™æŠ“å–å™¨"""
    
    def __init__(self, analyzer, api_key):
        super().__init__(analyzer)
        from googleapiclient.discovery import build  # type: ignore
        self.youtube = build('youtube', 'v3', developerKey=api_key)
    
    def fetch(self, query: str, limit: int = 5, region: str = "TW") -> list:
        results = []
        try:
            region_code = 'TW' if region == 'TW' else None 
            
            search_response = self.youtube.search().list(
                q=query, part='id', maxResults=limit, type='video', 
                order='date', regionCode=region_code
            ).execute()
            
            for item in search_response.get('items', []):
                video_id = item['id']['videoId']
                results.extend(self._fetch_video_comments(video_id))
                self._add_delay(1, 1)
        except Exception as e:
            st.error(f"YouTube æŠ“å–å¤±æ•—: {e}")
        return results
    
    def _fetch_video_comments(self, video_id: str, max_comments: int = 3) -> list:
        results = []
        try:
            comments = self.youtube.commentThreads().list(
                part='snippet', videoId=video_id, maxResults=max_comments, order='relevance'
            ).execute()
            
            for comment_item in comments.get('items', []):
                comment = comment_item['snippet']['topLevelComment']['snippet']
                result = self._create_result_dict(
                    content=comment['textDisplay'][:300], platform='YouTube',
                    url=f"https://youtube.com/watch?v={video_id}",
                    author=comment['authorDisplayName'], engagement=comment['likeCount']
                )
                results.append(result)
        except: pass
        return results


class GoogleNewsFetcher(DataFetcher):
    """Google News RSS æŠ“å–å™¨ (æ”¯æ´åœ°å€) - å„ªåŒ–ç‰ˆ"""
    
    def fetch(self, query: str, limit: int = 10, region: str = "TW") -> list:
        results = []
        try:
            # è¨­å®šèªç³»åƒæ•¸
            lang_params = "hl=zh-TW&gl=TW&ceid=TW:zh-Hant" if region == "TW" else "hl=en-US&gl=US&ceid=US:en"
            
            # å…§éƒ¨å‡½æ•¸ï¼šè™•ç† URL çµ„åˆèˆ‡æŠ“å–
            def _get_feed(search_query):
                encoded_query = quote(search_query)
                url = f"https://news.google.com/rss/search?q={encoded_query}&{lang_params}"
                return feedparser.parse(url)

            # å„ªå…ˆå˜—è©¦ç²¾ç¢ºæœå°‹
            feed = _get_feed(f'"{query}"')
            
            # è‹¥ç„¡çµæœï¼Œå˜—è©¦æ¨¡ç³Šæœå°‹
            if not feed.entries:
                st.warning(f"Google News ({region}) æœªæ‰¾åˆ°ç²¾ç¢ºçµæœï¼Œå˜—è©¦æ¨¡ç³Šæœå°‹...")
                feed = _get_feed(query)
            
            relevant_count = 0
            for entry in feed.entries:
                if relevant_count >= limit: break
                
                title = entry.get('title', '')
                summary = entry.get('summary', entry.get('description', ''))
                content = f"{title}\n{summary[:200]}" if summary else title
                
                if not content.strip() or not self._is_relevant(content, query, 0.2): continue
                
                results.append(self._create_result_dict(
                    content=content, platform='Google News',
                    url=entry.get('link', entry.get('url', '')),
                    author=entry.get('source', {}).get('title', 'Unknown')
                ))
                relevant_count += 1
                self._add_delay(0.5, 1)
                
            if not results: st.info(f"ğŸ” Google News ({region}) æœªæ‰¾åˆ°ç›¸é—œæ–°è")
            
        except Exception as e:
            st.error(f"Google News æŠ“å–å¤±æ•—: {str(e)}")
            
        return results


class DuckDuckGoFetcher(DataFetcher):
    """DuckDuckGo æœå°‹æŠ“å–å™¨ (æ”¯æ´åœ°å€) - å„ªåŒ–ç‰ˆ"""
    
    def __init__(self, analyzer):
        super().__init__(analyzer)
        self.last_request_time = 0
        self.min_interval = 3.5  # å¢åŠ é–“éš”é¿å…è¢«å°é–
    
    def _rate_limit_wait(self):
        elapsed = time.time() - self.last_request_time
        if elapsed < self.min_interval:
            time.sleep(self.min_interval - elapsed)
        self.last_request_time = time.time()
    
    def fetch(self, query: str, limit: int = 10, region: str = "TW", search_type: str = "news") -> list:
        results = []
        max_retries = 3
        ddg_region = 'tw-tz' if region == 'TW' else 'wt-wt'
        
        try:
            self._rate_limit_wait()
            with DDGS() as ddgs:
                search_results = []
                # å®šç¾©æœå°‹ç­–ç•¥ï¼š[ç²¾ç¢ºæœå°‹, æ¨¡ç³Šæœå°‹]
                search_strategies = [f'"{query}"', query]
                
                for attempt in range(max_retries):
                    try:
                        if attempt > 0: st.info(f"ğŸ”„ DDG é‡è©¦ä¸­... ({attempt+1})")
                        
                        # è¿´åœˆå˜—è©¦ä¸åŒç­–ç•¥ï¼Œæ¸›å°‘ if/else å·¢ç‹€
                        for q_str in search_strategies:
                            if search_type == "news":
                                res = list(ddgs.news(
                                    query=q_str, region=ddg_region, safesearch='moderate', 
                                    timelimit='y', max_results=min(limit + 5, 15)  # æ¸›å°‘è«‹æ±‚é‡
                                ))
                            else:
                                res = list(ddgs.text(
                                    query=q_str, region=ddg_region, safesearch='moderate', 
                                    backend='html', max_results=min(limit + 5, 15)  # æ¸›å°‘è«‹æ±‚é‡
                                ))
                            
                            if res:
                                search_results = res
                                break # æˆåŠŸç²å¾—çµæœ
                        
                        if search_results: break # è·³å‡ºé‡è©¦è¿´åœˆ
                        
                        time.sleep(3.0 + random.uniform(0, 1.5))  # éš¨æ©Ÿå»¶é²é¿å…è¦å¾‹æ€§
                        
                    except Exception as e:
                        st.warning(f"âš ï¸ DDG å˜—è©¦ {attempt+1} å¤±æ•—: {str(e)[:80]}")
                        # æœ€å¾Œä¸€æ¬¡å˜—è©¦è‹¥æ˜¯ News å¤±æ•—ï¼Œé™ç´šç‚º Text (Fallback)
                        if attempt == max_retries - 1 and search_type == "news":
                            try:
                                search_results = list(ddgs.text(
                                    query=query, region=ddg_region, backend='html', max_results=min(limit + 5, 15)
                                ))
                            except Exception as fallback_err:
                                st.error(f"âŒ Fallback ä¹Ÿå¤±æ•—: {str(fallback_err)[:50]}")
                        time.sleep(3.0 + random.uniform(0, 1.5))

                # è™•ç†çµæœ
                relevant_count = 0
                for item in search_results:
                    if relevant_count >= limit: break
                    
                    title = item.get('title', '')
                    body = item.get('body', '')
                    content = f"{title}\n{body}"[:500]
                    url = item.get('url', item.get('href', ''))
                    
                    if not content.strip() or not url: continue
                    if not self._is_relevant(content, query, 0.1):  # é™ä½é–€æª»å¾ 0.2 åˆ° 0.1
                        continue
                    
                    results.append(self._create_result_dict(
                        content=content, platform='DuckDuckGo',
                        url=url, author=item.get('source', 'Web')
                    ))
                    relevant_count += 1
            
            # æä¾›æœå°‹çµæœçµ±è¨ˆ
            if search_results and not results:
                st.warning(f"âš ï¸ DDG å›å‚³ {len(search_results)} ç­†ï¼Œä½†ç„¡ç¬¦åˆç›¸é—œæ€§çš„çµæœ")
            elif not search_results:
                st.warning("âš ï¸ DDG æœªå›å‚³ä»»ä½•çµæœï¼Œå¯èƒ½è¢«é™æµæˆ–æŸ¥è©¢éæ–¼é »ç¹")
                    
        except Exception as e:
            st.error(f"âŒ DDG é€£ç·šéŒ¯èª¤: {str(e)[:120]}")
        
        return results


# ============================================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šè³‡æ–™æŠ“å–å”èª¿å™¨
# ============================================================================

class DataCoordinator:
    """å”èª¿å¤šå€‹è³‡æ–™æºçš„æŠ“å–ä½œæ¥­"""
    
    def __init__(self, analyzer: UnifiedAIAnalyzer, config: Config):
        self.analyzer = analyzer
        self.config = config
        self.fetchers = self._initialize_fetchers()
    
    def _initialize_fetchers(self) -> dict:
        fetchers = {}
        fetchers['Google News'] = GoogleNewsFetcher(self.analyzer)
        fetchers['DuckDuckGo'] = DuckDuckGoFetcher(self.analyzer)
        
        if Config.REDDIT_AVAILABLE and self.config.REDDIT_CLIENT_ID:
            try:
                fetchers['Reddit'] = RedditFetcher(
                    self.analyzer, self.config.REDDIT_CLIENT_ID,
                    self.config.REDDIT_CLIENT_SECRET, self.config.REDDIT_USER_AGENT
                )
            except: pass
        
        if Config.YOUTUBE_AVAILABLE and self.config.YOUTUBE_API_KEY:
            try:
                fetchers['YouTube'] = YouTubeFetcher(self.analyzer, self.config.YOUTUBE_API_KEY)
            except: pass
        
        return fetchers
    
    def fetch_all(self, query: str, sources: list, items_per_source: int, region: str) -> list:
        all_results = []
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for idx, source in enumerate(sources):
            status_text.text(f"æ­£åœ¨æŠ“å– {source} (åœ°å€: {region})...")
            
            if source in self.fetchers:
                results = self.fetchers[source].fetch(query, items_per_source, region=region)
                all_results.extend(results)
            
            progress_bar.progress((idx + 1) / len(sources))
        
        status_text.empty()
        progress_bar.empty()
        return all_results


# ============================================================================
# ç¬¬å…­éƒ¨åˆ†ï¼šUI ä»‹é¢
# ============================================================================

class StreamlitUI:
    """Streamlit ä½¿ç”¨è€…ä»‹é¢"""
    
    def __init__(self):
        st.set_page_config(page_title="ä¼æ¥­ã€å“ç‰ŒåŠå€‹äººè²è­½æ™ºæ…§ç›£æ¸¬ V1.1", layout="wide", page_icon="ğŸ›¡ï¸")
        self.db = DatabaseManager()
        self.config = self._setup_sidebar()
        
        # åˆå§‹åŒ– AI åˆ†æå™¨
        self.analyzer = None
        self.coordinator = None
        
        provider = st.session_state.get('llm_provider', 'Gemini')
        api_key = self.config.GEMINI_API_KEY if provider == 'Gemini' else self.config.OPENAI_API_KEY
        
        if api_key:
            self.analyzer = UnifiedAIAnalyzer(provider.lower(), api_key)
            self.coordinator = DataCoordinator(self.analyzer, self.config)
    
    def _setup_sidebar(self) -> Config:
        with st.sidebar:
            st.header("âš™ï¸ ç³»çµ±è¨­å®š")
            
            # --- LLM é¸æ“‡ ---
            st.subheader("1. AI æ¨¡å‹é¸æ“‡")
            llm_provider = st.radio(
                "é¸æ“‡åˆ†ææ¨¡å‹", 
                ["Gemini", "OpenAI"],
                key="llm_provider",
                help="Gemini é©åˆå¤§é‡å…è²»åˆ†æï¼›OpenAI éœ€ä»˜è²»ä½†ç©©å®šæ€§é«˜"
            )
            
            if llm_provider == "Gemini":
                Config.GEMINI_API_KEY = st.text_input("Gemini API Key", type="password")
            else:
                if not Config.OPENAI_AVAILABLE:
                    st.error("è«‹å…ˆå®‰è£ï¼š`pip install openai`")
                Config.OPENAI_API_KEY = st.text_input("OpenAI API Key", type="password")
                
            st.divider()
            
            # --- è³‡æ–™æº Key ---
            st.subheader("2. è³‡æ–™æºæˆæ¬Š")
            with st.expander("Reddit / YouTube è¨­å®š"):
                Config.REDDIT_CLIENT_ID = st.text_input("Reddit Client ID", type="password")
                Config.REDDIT_CLIENT_SECRET = st.text_input("Reddit Client Secret", type="password")
                Config.YOUTUBE_API_KEY = st.text_input("YouTube API Key", type="password")
            
            st.divider()
            if st.button("ğŸ—‘ï¸ æ¸…ç©ºè³‡æ–™åº«", type="secondary"):
                self.db.clear_all_data()
                st.success("å·²æ¸…ç©º")
                st.rerun()
        
        return Config
    
    def render_search_form(self) -> tuple:
        st.info("ğŸ’¡ æç¤ºï¼šé¸æ“‡ã€Œå°ç£ã€å°‡é–å®šç¹é«”ä¸­æ–‡èˆ‡æœ¬åœ°æ–°èï¼›ã€Œåœ‹éš›ã€å°‡æœå°‹å…¨çƒè‹±æ–‡/å¤–æ–‡è³‡è¨Šã€‚")
        
        col1, col2 = st.columns([2, 1])
        with col1:
            query = st.text_input("ğŸ” ç›£æ§é—œéµå­—", value="å°åŒ—æ·é‹", placeholder="è¼¸å…¥å…¬å¸åç¨±")
        with col2:
            region = st.radio("ğŸŒ æœå°‹å€åŸŸ", ["å°ç£ (TW)", "åœ‹éš› (Global)"], horizontal=True)
            region_code = "TW" if "å°ç£" in region else "Global"

        col3, col4 = st.columns([2, 1])
        with col3:
            available_sources = ["Google News", "DuckDuckGo"]
            if Config.REDDIT_AVAILABLE: available_sources.insert(0, "Reddit")
            if Config.YOUTUBE_AVAILABLE: available_sources.insert(1, "YouTube")
            data_sources = st.multiselect("è³‡æ–™ä¾†æº", available_sources, default=["Google News", "DuckDuckGo"])
        with col4:
            items_per_source = st.number_input("æ¯ä¾†æºç­†æ•¸", 3, 20, 5)
            
        return query, data_sources, items_per_source, region_code
    
    def render_dashboard(self, df: pd.DataFrame):
        if df.empty:
            st.info("ğŸ‘† è«‹é»æ“Šä¸Šæ–¹æŒ‰éˆ•é–‹å§‹ç›£æ¸¬")
            return
        
        # å ±å‘Šå€å¡Š
        st.header(f"ğŸ“Š AI æ™ºæ…§åˆ†æå ±å‘Š ({st.session_state.get('llm_provider', 'Unknown')})")
        with st.spinner("ç”Ÿæˆä¸­..."):
            if self.analyzer:
                summary = self.analyzer.generate_summary_report(df)
                st.markdown(summary)
            else:
                st.warning("âš ï¸ è«‹å…ˆè¨­å®š API Key")
        
        # é—œéµæŒ‡æ¨™
        col1, col2, col3, col4 = st.columns(4)
        with col1: st.metric("å¹³å‡åˆ†æ•¸", f"{df['sentiment_score'].mean():.2f}")
        with col2: st.metric("æ­£é¢", len(df[df['sentiment_label'] == 'æ­£é¢']))
        with col3: st.metric("è² é¢", len(df[df['sentiment_label'] == 'è² é¢']))
        with col4: st.metric("äº’å‹•æ•¸", f"{df['engagement'].sum():,}")
        
        # åœ–è¡¨
        c1, c2 = st.columns(2)
        with c1: st.bar_chart(df['platform'].value_counts())
        with c2: st.bar_chart(df['sentiment_label'].value_counts())
        
        # è³‡æ–™è¡¨
        st.dataframe(df[['timestamp', 'platform', 'sentiment_label', 'content', 'url']], 
                     column_config={"url": st.column_config.LinkColumn("é€£çµ")}, use_container_width=True)

    def run(self):
        st.title("ğŸ›¡ï¸ ä¼æ¥­ã€å“ç‰ŒåŠå€‹äººè²è­½æ™ºæ…§ç›£æ¸¬ V1.1")
        st.caption("ç‰ˆæœ¬ï¼šGemini æ”¯æ´ | OpenAI æ”¯æ´ | åœ°å€åˆ‡æ› | Google News | DuckDuckGo | Reddit | YouTube")
        
        query, data_sources, items_per_source, region = self.render_search_form()
        
        if st.button("ğŸš€ é–‹å§‹æ™ºæ…§ç›£æ¸¬", type="primary"):
            provider = st.session_state.get('llm_provider')
            has_key = Config.GEMINI_API_KEY if provider == 'Gemini' else Config.OPENAI_API_KEY
            
            if not has_key:
                st.error(f"âš ï¸ è«‹å…ˆåœ¨å´é‚Šæ¬„è¼¸å…¥ {provider} API Key")
            else:
                with st.spinner(f"ğŸ¤– {provider} AI æ­£åœ¨æœå°‹ ({region}) ä¸¦åˆ†æ..."):
                    all_results = self.coordinator.fetch_all(
                        query, data_sources, items_per_source, region
                    )
                    for item in all_results:
                        self.db.save_mention(
                            item['content'], item['platform'], item['score'],
                            item['label'], item['url'], item['author'], item['engagement']
                        )
                    st.success(f"âœ… å®Œæˆï¼å…±åˆ†æ {len(all_results)} ç­†è³‡æ–™")
                    st.rerun()
        
        df = self.db.get_recent_mentions(limit=100)
        self.render_dashboard(df)

if __name__ == "__main__":
    app = StreamlitUI()
    app.run()